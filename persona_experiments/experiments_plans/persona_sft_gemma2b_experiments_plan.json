{
  "name": "Persona initial SFT phase for the Gemma-2B model",
  "description": "",
  "skip": 0,
  "repetitions": 1,
  "largest": false,
  "multiprocess": false,
  "num_parallel": 1,
  "gpu_ids_pool": [],
  "configurations": [
    {
      "base_config": {
        "experiment_name": "gemma2b_sft_persona_{objective}",
        "random_seed": -1,
        "gpu_ids": [
          0
        ],
        "trainer_checkpoint": "",
        "epochs": 1,
        "validate_every": 1,
        "outputs_dir": "outputs/persona_models",
        "disable_console_log": true,
        "save_logs": true,
        "train_batch_log_interval": 5,
        "epoch_log_interval": 1,
        "save_metric_plots": false,
        "save_every_num_val": 1,
        "use_tensorboard": false,
        "use_wandb": false,
        "wandb_resume_path": "",
        "wandb_project_name": "persona_likelihood_displacement",
        "wandb_entity_name": "",
        "wandb_track_model": null,
        "wandb_exclude_files": [
          "plots/**"
        ],
        "score_metric_name": "train loss",
        "is_train_metric": true,
        "score_largest": false,
        "return_best_score": false,
        "save_checkpoints": false,
        "num_checkpoints": 1,
        "save_checkpoints_by_score": false,
        "early_stop": false,
        "early_stop_min_delta": 0,
        "early_stop_patience": 0,
        "early_stop_cooldown": 0,
        "early_stop_restore_best_weights": false,
        "dataset": "data_files/persona/ends-justify-means.jsonl",
        "num_train_samples": -1,
        "answer_matching_behavior_to_use": "",
        "train_samples_random_seed": -1,
        "batch_size": 32,
        "output_tokens_matching_yes": [
          "No",
          "Never",
          "Maybe",
          "Perhaps"
        ],
        "output_tokens_matching_no": [
          "Yes",
          "Yeah",
          "Sure",
          "Certainly",
          "Absolutely"
        ],
        "load_dataset_to_gpu": true,
        "model": "google/gemma-2b",
        "model_cache_dir": null,
        "load_model_checkpoint": null,
        "is_lora_checkpoint": false,
        "use_lora": false,
        "lora_rank": 8,
        "kl_coeff": 0.01,
        "objective": "cross_entropy",
        "optimizer": "rmsprop",
        "lr": 1e-7,
        "track_logits_for_tokens": [],
        "log_top_token_logit_change_interval": -1,
        "save_model": true,
        "save_finegrained_token_metrics": false
      },
      "options": {}
    }
  ]
}